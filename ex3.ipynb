{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 3:\n",
    "\n",
    "***Neural Networks and Gaussian process:***\n",
    "Predict the SP500 with the  nancial indicators assigned to your team in the google spreadsheet (ep, dp, de, dy, dfy, bm, svar, ntis, in, tbl , see RLab3 2 GWcausalSP500.R), some lagged series of these indicators and lags of the target using a Neural Network and a GP regression with your desired kernel. Predict return, or price, or direction (up or down). For which target works best? Do some feature selection to disregard some variables, select appropriate lags: causality, (distance) correlation, VAR-test, Lasso ... (The script RLab5 GausProc.R can be of help. The dataset is goyal-welch2022Monthly.csv and work within the period 1927/2021.)\n",
    "\n",
    "In our case, we have been assigned variables dp, de and ep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEX:\n",
    "\n",
    "0. [DATA AND LIBRARY IMPORTS](#0.-DATA-AND-LIBRARY-IMPORTS)\n",
    "\n",
    "1. [PREPROCESSING AND FEATURE ENGINEERING](#1.-PREPROCESSING-AND-FEATURE-ENGINEERING)\n",
    "\n",
    "2. [LAG CREATION AND SELECTION](#2.-LAG-CREATION-AND-SELECTION)\n",
    "\n",
    "3. [ARIMA BASELINE MODEL](#3.-ARIMA-BASELINE-MODEL)\n",
    "\n",
    "4. [NEURAL NETWORK](#3.-NEURAL-NETWORK)\n",
    "\n",
    "5. [GAUSSIAN PROCESS](#4.-GAUSSIAN-PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. DATA AND LIBRARY IMPORTS\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, acf, pacf\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.stats import chi2\n",
    "from scipy.spatial.distance import correlation\n",
    "import scipy.stats\n",
    "import dcor\n",
    "from sklearn.linear_model import Lasso\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "After some further steps on the way down, we decided to keep the last period from 1921 for now, as it will be useful for havig a first value for the differenced variables straight from the beginning. We will also use the previous periods in order to be able to have some lags for the variables straight from the beginning of 1927."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp = pd.read_csv('goyal-welch2022Monthly.csv')\n",
    "\n",
    "snp['yyyymm'] = snp['yyyymm'].astype(str)\n",
    "snp['yyyymm'] = pd.to_datetime(snp['yyyymm'], format='%Y%m')\n",
    "snp = snp.loc[(snp['yyyymm'] >= '1921-01-01') & (snp['yyyymm'] < '2022-01-01')].reset_index(drop=True)\n",
    "snp['Index'] = snp['Index'].str.replace(',', '').astype(float)\n",
    "\n",
    "display(snp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PREPROCESSING AND FEATURE ENGINEERING\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values and Corrupted Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that THERE ARE NO MISSING VALUES FOR OUR VARIABLES OF INTEREST (Index, D12 and E12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the new features as we need to compute:\n",
    "\n",
    "- Dividend Price Ratio (DP)\n",
    "- Dividend Earnings Ratio (DE)\n",
    "- Earnings Price Ratio (EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp = snp[['yyyymm', 'Index', 'D12', 'E12']]\n",
    "\n",
    "snp['LogReturns'] = np.log(snp['Index']).diff()\n",
    "snp['PriceDiv'] = snp['Index'] + snp['D12']\n",
    "snp['LogReturnsDiv'] = np.log(snp['PriceDiv']).diff()\n",
    "\n",
    "# We need to fill the NaN values with 0 because the ADF test doesn't tolerate NaN values and we might still need to further differentiate the series.\n",
    "snp.fillna({'LogReturns': 0}, inplace=True)\n",
    "snp.fillna({'LogReturnsDiv': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp['DP'] = np.log(snp['D12']) - np.log(snp['Index'])\n",
    "snp['DE'] = np.log(snp['D12']) - np.log(snp['E12'])\n",
    "snp['EP'] = np.log(snp['E12']) - np.log(snp['Index'])\n",
    "\n",
    "display(snp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity and Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=9, ncols=1, figsize=(10, 18))\n",
    "\n",
    "columns_to_plot = ['Index', 'PriceDiv', 'D12', 'E12', 'LogReturns', 'LogReturnsDiv', 'DP', 'DE', 'EP']\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    axes[i].plot(snp['yyyymm'], snp[col], marker='', linestyle='-')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test for stationarity in the data\n",
    "\n",
    "for col in columns_to_plot:\n",
    "    adf_result = adfuller(snp[col])\n",
    "    print(f'ADF Statistic for {col}: {adf_result[0]}')\n",
    "    print(f'p-value for {col}: {adf_result[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables we are interested in are mostly stationary but there are a couple that should be further differenced in order to make them stationary.\n",
    "\n",
    "That being, said, log returns and log returns + dividends are already quite surely stationary so we're not going to bother with their stationarity anymore.\n",
    "\n",
    "Additionally, in further boxplots we have been able to see that variables DE and EP are quite skewed so, even if they are already decently stationary, we will also difference them in order to center them a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp['DP'] = snp['DP'].diff()\n",
    "snp['DE'] = snp['DE'].diff()\n",
    "snp['EP'] = snp['EP'].diff()\n",
    "\n",
    "display(snp.head())\n",
    "\n",
    "# We can safely remove the first row now instead of filling it with 0\n",
    "snp.dropna(axis = 0, inplace = True)\n",
    "\n",
    "display(snp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 6))\n",
    "\n",
    "columns_to_plot = ['DP', 'DE', 'EP']\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    axes[i].plot(snp['yyyymm'], snp[col], marker='', linestyle='-')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We test again for stationarity in the data\n",
    "\n",
    "for col in columns_to_plot:\n",
    "    adf_result = adfuller(snp[col])\n",
    "    print(f'ADF Statistic for {col}: {adf_result[0]}')\n",
    "    print(f'p-value for {col}: {adf_result[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how now our data is completely stationary and we may proceed. Careful attention to the outliers will be needed, though, as they are very specific to the 2008 crisis. We will first standardize the data and afterwards, if there are still outliers, we will treat them or maybe consider scaling with robust scaling (i.e. taking away the median instead of the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = ['LogReturns', 'LogReturnsDiv', 'DP', 'DE', 'EP']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 10))\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    sns.boxplot(y=col, data=snp, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {col}', fontsize=12)\n",
    "    axes[i].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "snp_std = snp.copy()\n",
    "\n",
    "for col in columns_to_plot:\n",
    "    snp_std[col] = robust_scaler.fit_transform(snp_std[[col]])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 10))\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    sns.boxplot(y=col, data=snp, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {col}', fontsize=12)\n",
    "    axes[i].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the scaled version, even with the robust scaler, returns ranges which are still bigger than the range from the previous version. Even with the outliers we had, as we had already applied a logarithmic transformation to the data, the outliers were not excessively far away from the center, with the exception of the DE and EP ratios.\n",
    "\n",
    "What we will do is \"manually\" transform the data which is bigger than 0.3 in the five columns we are considering. We will first try to simply perform a sort of winsorization on those values to the nearest value that we establish as \"the maximum\" we allow. As such, we simply set the values exceeding ±0.3, to ±0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_to_plot:\n",
    "    snp[col] = snp[col].clip(upper=0.3, lower=-0.3)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 10))\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    sns.boxplot(y=col, data=snp, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {col}', fontsize=12)\n",
    "    axes[i].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(10, 10))\n",
    "\n",
    "columns_to_plot = ['LogReturns', 'LogReturnsDiv', 'DP', 'DE', 'EP']\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    axes[i].plot(snp['yyyymm'], snp[col], marker='', linestyle='-')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LAG CREATION AND SELECTION\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = ['LogReturns', 'LogReturnsDiv', 'DP', 'DE', 'EP']\n",
    "\n",
    "fig, axes = plt.subplots(10, 1, figsize=(10, 20))\n",
    "\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    acf_index = 2 * i\n",
    "    plot_acf(snp[col], lags = 100, alpha = 0.05, ax = axes[acf_index], title = f'ACF for {col}')\n",
    "    axes[acf_index].grid(True)\n",
    "    \n",
    "    pacf_index = 2 * i + 1\n",
    "    plot_pacf(snp[col], lags = 100, alpha = 0.05, ax = axes[pacf_index], title = f'PACF for {col}')\n",
    "    axes[pacf_index].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how there is no clear patter in the ACF and PACF plots for most of the variables, so we will just start using an arbitrary number of lags and try different options from there. That being said, there is some lag correlation for DE and EP so those lags are probably going to be the most relevant, although with 5-10 lags we will already cover these correlations.\n",
    "\n",
    "Let's perform some causality tests to see how many lags we should consider for each variable. As we have already seen from the ACF and PACF that the maximum amount of relevant lags is less than 10, we will consider only 5 lags for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_lags = snp.copy()\n",
    "snp_lags.drop(['Index', 'D12', 'E12', 'PriceDiv'], axis = 1, inplace = True)\n",
    "display(snp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [   [\"1927-01-01\", \"1932-12-01\"],\n",
    "                [\"1933-01-01\", \"1970-12-01\"],\n",
    "                [\"1971-01-01\", \"1997-12-01\"],\n",
    "                [\"1998-01-01\", \"2005-12-01\"],\n",
    "                [\"2006-01-01\", \"2021-11-01\"]]\n",
    "\n",
    "causality_tests = []\n",
    "\n",
    "for interval in intervals:\n",
    "    snp_temp = snp_lags[(snp_lags['yyyymm'] >= interval[0]) & (snp_lags['yyyymm'] <= interval[1])].dropna().reset_index(drop=True)\n",
    "    snp_temp = snp_temp.drop('yyyymm', axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1],'\\n p = 5 \\n')\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1],'\\n p = 5 \\n')\n",
    "    \n",
    "    print(\"\\n\\nFor lagged LogReturns\\n\", '#'*20)\n",
    "    result = grangercausalitytests(snp_temp[[\"LogReturns\", \"LogReturns\"]], 5)\n",
    "    causality_tests.append(result)\n",
    "    \n",
    "    print(\"\\n\\nFor lagged DP\\n\", '#'*20)\n",
    "    result = grangercausalitytests(snp_temp[[\"LogReturns\", \"DP\"]], 5)\n",
    "    causality_tests.append(result)\n",
    "    \n",
    "    print(\"\\n\\nFor lagged DE\\n\", '#'*20)\n",
    "    result = grangercausalitytests(snp_temp[[\"LogReturns\", \"DE\"]], 5)\n",
    "    causality_tests.append(result)\n",
    "    \n",
    "    print(\"\\n\\nFor lagged EP\\n\", '#'*20)\n",
    "    result = grangercausalitytests(snp_temp[[\"LogReturns\", \"EP\"]], 5)\n",
    "    causality_tests.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how it appears that the EP ratio tends to be the most causal for LogReturns, especially in some periods more than others. Then we can also see that some lags of the DE ratio also pass the test (although only at a 10% confidence interval). As a result we are going to keep evaluating the lags to use, but we can probably safely try to use more than just 5 lags for DE and EP ratios.\n",
    "\n",
    "That being said, the results are not too surprising because the Dividends are just one component of the price, but a lot of people value more highly a stock which keeps its value and grows in price more than another stock which returns dividends but doesn't grow as much, because you can always resell the stock if needed.\n",
    "As a result, a company which is having consistent earnings is more probably higher valued than another with lower earnings and, as such, its price will probably be higher as well, hence this result. If a company does well one month, probably a lot of people will be at least a little bit more interested in buying their stock. The same can be said about an index like the S&P500. It's better to know that they are getting consistent and significant earnings more than knowing whether they paid dividends or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Correlation and Correlation among Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in intervals:\n",
    "    snp_temp = snp_lags[(snp_lags['yyyymm'] >= interval[0]) & (snp_lags['yyyymm'] <= interval[1])].dropna()\n",
    "    snp_temp = snp_temp.drop(['yyyymm', 'LogReturns', 'LogReturnsDiv'], axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    \n",
    "    dp_de_correlation = dcor.distance_correlation(snp_temp['DP'], snp_temp['DE'])\n",
    "    dp_ep_correlation = dcor.distance_correlation(snp_temp['DP'], snp_temp['EP'])\n",
    "    de_ep_correlation = dcor.distance_correlation(snp_temp['DE'], snp_temp['EP'])\n",
    "    \n",
    "    print(\"Distance correlation between DP and DE:\", dp_de_correlation)\n",
    "    print(\"Distance correlation between DP and EP:\", dp_ep_correlation)\n",
    "    print(\"Distance correlation between DE and EP:\", de_ep_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in intervals:\n",
    "    snp_temp = snp_lags[(snp_lags['yyyymm'] >= interval[0]) & (snp_lags['yyyymm'] <= interval[1])].dropna()\n",
    "    snp_temp = snp_temp.drop(['yyyymm', 'LogReturns', 'LogReturnsDiv'], axis = 1)\n",
    "    corr = snp_temp.corr()\n",
    "    display(corr.style.background_gradient(cmap='coolwarm', axis=None).format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Columns Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to conduct LASSO analysis, we have to already have the variables created. We have to create the lags for the variables so we will analyze up until 20 lags for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_corr_matrix = columns_to_plot.copy()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    snp[f'Return_lag_{i}'] = snp['LogReturns'].shift(i)\n",
    "    snp[f'ReturnsDiv_lag_{i}'] = snp['LogReturnsDiv'].shift(i)\n",
    "    snp[f'DP_lag_{i}'] = snp['DP'].shift(i)\n",
    "    snp[f'DE_lag_{i}'] = snp['DE'].shift(i)\n",
    "    snp[f'EP_lag_{i}'] = snp['EP'].shift(i)\n",
    "    \n",
    "    cols_corr_matrix.append(f'Return_lag_{i}')\n",
    "    cols_corr_matrix.append(f'ReturnsDiv_lag_{i}')\n",
    "    cols_corr_matrix.append(f'DP_lag_{i}')\n",
    "    cols_corr_matrix.append(f'DE_lag_{i}')\n",
    "    cols_corr_matrix.append(f'EP_lag_{i}')\n",
    "\n",
    "display(snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp = snp.loc[(snp['yyyymm'] >= '1927-01-01') & (snp['yyyymm'] < '2022-01-01')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = snp[cols_corr_matrix].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None).format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the Log Returns are not super correlated with anything but are actually a little bit correlated (less than 10%) with some things like some of its own lags and some lags of the DP and EP ratios as well. That being said, obviously we cannot really use the most correlated variables, which are the lag 0 ratios because they are directly computed using the Price so we would not have them in the actual period we will be trying to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_lasso = snp.copy()\n",
    "\n",
    "snp_lasso.drop(['LogReturnsDiv', 'DP', 'DE', 'EP'], axis = 1, inplace = True)\n",
    "\n",
    "display(snp_lasso)\n",
    "\n",
    "for interval in intervals:\n",
    "    snp_temp = snp_lasso[(snp_lasso['yyyymm'] >= interval[0]) & (snp_lasso['yyyymm'] <= interval[1])].dropna().copy()\n",
    "    snp_temp = snp_temp.drop(['yyyymm'], axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    \n",
    "    y = snp_temp['LogReturns'].copy()\n",
    "    X = snp_temp.drop(['LogReturns'], axis = 1)\n",
    "    \n",
    "    lasso_model = Lasso(alpha=0.01)\n",
    "    lasso_model.fit(X, y)\n",
    "    print(\"Lasso coefficients:\", lasso_model.coef_)\n",
    "    print(\"Lasso intercept:\", lasso_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teras Virta Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terasvirta_test(x, y):\n",
    "    model = Sequential()\n",
    "    input_dim = 1 if len(x.shape) == 1 else x.shape[1]\n",
    "    model.add(Dense(2, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(x, y, epochs=50, verbose=False)\n",
    "    \n",
    "    linear_model = Sequential()\n",
    "    linear_model.add(Dense(1, activation='linear'))\n",
    "    linear_model.compile(loss='mse', optimizer='sgd')\n",
    "    linear_model.fit(x, y, epochs=50, verbose=False)\n",
    "    pred_orig = linear_model.predict(x)\n",
    "    resid_orig = y.values - pred_orig\n",
    "    pred_nn = model.predict(x)\n",
    "    pred_nn = pred_nn.reshape(-1)\n",
    "    resid_nn = y.values - pred_nn\n",
    "    test_stat = np.mean(resid_orig**2 - resid_nn**2)\n",
    "    crit_val = chi2.ppf(0.95, 2)\n",
    "    if test_stat > crit_val:\n",
    "        print(\"The null hypothesis of linearity is rejected\")\n",
    "    else:\n",
    "        print(\"The null hypothesis of linearity is not rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_teras = snp.copy()\n",
    "\n",
    "for interval in intervals:\n",
    "    snp_temp = snp_teras[(snp_teras['yyyymm'] >= interval[0]) & (snp_teras['yyyymm'] <= interval[1])].dropna().copy()\n",
    "    snp_temp = snp_temp.drop(['yyyymm', 'LogReturnsDiv'], axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    \n",
    "    y = snp_temp['LogReturns'].copy()\n",
    "    X = snp_temp['DP'].copy()\n",
    "    \n",
    "    terasvirta_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in intervals:\n",
    "    snp_temp = snp_teras[(snp_teras['yyyymm'] >= interval[0]) & (snp_teras['yyyymm'] <= interval[1])].dropna().copy()\n",
    "    snp_temp = snp_temp.drop(['yyyymm', 'LogReturnsDiv'], axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    \n",
    "    y = snp_temp['LogReturns'].copy()\n",
    "    X = snp_temp['DE'].copy()\n",
    "    \n",
    "    terasvirta_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in intervals:\n",
    "    snp_temp = snp_teras[(snp_teras['yyyymm'] >= interval[0]) & (snp_teras['yyyymm'] <= interval[1])].dropna().copy()\n",
    "    snp_temp = snp_temp.drop(['yyyymm', 'LogReturnsDiv'], axis = 1)\n",
    "    if interval == intervals[0]:\n",
    "        print(interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n',interval[0],' , ',interval[1], '\\n', '#'*20)\n",
    "    \n",
    "    y = snp_temp['LogReturns'].copy()\n",
    "    X = snp_temp['EP'].copy()\n",
    "    \n",
    "    terasvirta_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['DP', 'DE', 'EP']:\n",
    "    snp_temp = snp_teras.dropna().copy()\n",
    "    if i == ['DP']:\n",
    "        print(i, '\\n', '#'*20)\n",
    "    else:\n",
    "        print('\\n\\n', i, '\\n', '#'*20)\n",
    "    \n",
    "    y = snp_temp['LogReturns'].copy()\n",
    "    X = snp_temp[i].copy()\n",
    "    \n",
    "    terasvirta_test(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ARIMA Baseline Model\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to forecast whether the stock will go up, down or will stay more or less the same. We will use the log returns to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = snp['LogReturns'].std()\n",
    "threshold = 0.05 * std_dev\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_returns(x):\n",
    "    \"\"\"\n",
    "    This function categorizes the returns into 1 (Up), -1 (Down) or 0 (less than a 5% of the standard deviation of the returns, i.e. no significant movement)\n",
    "    \"\"\"\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    elif x < -threshold:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp['Target'] = snp['LogReturns'].apply(categorize_returns)\n",
    "\n",
    "display(snp.head())\n",
    "print(snp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = snp['Target'].copy()\n",
    "X = snp.drop(['yyyymm', 'Index', 'D12', 'E12', 'PriceDiv', 'LogReturns', 'LogReturnsDiv', 'Target', 'DP', 'DE', 'EP'], axis = 1).copy()\n",
    "\n",
    "display(y.head())\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # One time step (Assuming input x is already batched)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])  # just need the last time step output\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X.shape[1]  # Number of input features (number of lags)\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3  # Three classes: up, stay, down\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(50):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NEURAL NETWORK\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. GAUSSIAN PROCESS\n",
    "\n",
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT IN USE, SAFEKEEPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distance_matrix(vector):\n",
    "#     return np.abs(vector[:, None] - vector[None, :])\n",
    "\n",
    "# def distance_covariance(X, Y):\n",
    "#     n = X.shape[0]\n",
    "#     A = distance_matrix(X)\n",
    "#     B = distance_matrix(Y)\n",
    "#     A_mean = A.mean()\n",
    "#     B_mean = B.mean()\n",
    "#     A_centered = A - A_mean\n",
    "#     B_centered = B - B_mean\n",
    "#     dcov = np.sqrt((A_centered * B_centered).sum() / (n * n))\n",
    "#     return dcov\n",
    "\n",
    "# def distance_correlation(X, Y):\n",
    "#     dcov_XY = distance_covariance(X, Y)\n",
    "#     dcov_XX = distance_covariance(X, X)\n",
    "#     dcov_YY = distance_covariance(Y, Y)\n",
    "#     if dcov_XX * dcov_YY == 0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return dcov_XY / np.sqrt(dcov_XX * dcov_YY)\n",
    "\n",
    "# for interval in intervals:\n",
    "#     snp_temp = snp_lags[(snp_lags['yyyymm'] >= interval[0]) & (snp_lags['yyyymm'] <= interval[1])].dropna()\n",
    "#     snp_temp = snp_temp.drop(['yyyymm', 'LogReturns', 'LogReturnsDiv'], axis = 1)\n",
    "#     print('\\n\\n',interval[0],' , ',interval[1],'\\n', '#'*20)\n",
    "    \n",
    "#     dp_de_corr = distance_correlation(snp_temp['DP'].values, snp_temp['DE'].values)\n",
    "#     dp_ep_corr = distance_correlation(snp_temp['DP'].values, snp_temp['EP'].values)\n",
    "#     de_ep_corr = distance_correlation(snp_temp['DE'].values, snp_temp['EP'].values)\n",
    "    \n",
    "#     print(\"Distance correlation between DP and DE:\", dp_de_corr)\n",
    "#     print(\"Distance correlation between DP and EP:\", dp_ep_corr)\n",
    "#     print(\"Distance correlation between DE and EP:\", de_ep_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interval in intervals:\n",
    "#     snp_temp = snp_lags[(snp_lags['yyyymm'] >= interval[0]) & (snp_lags['yyyymm'] <= interval[1])].dropna()\n",
    "#     snp_temp = snp_temp.drop(['yyyymm', 'LogReturns', 'LogReturnsDiv'], axis = 1)\n",
    "#     if interval == intervals[0]:\n",
    "#         print(interval[0],' , ',interval[1],'\\n', '#'*20)\n",
    "#     else:\n",
    "#         print('\\n\\n',interval[0],' , ',interval[1],'\\n', '#'*20)\n",
    "    \n",
    "#     dp_de_corr_dist = correlation(snp_temp['DP'].values, snp_temp['DE'].values)\n",
    "#     dp_ep_corr_dist = correlation(snp_temp['DP'].values, snp_temp['EP'].values)\n",
    "#     de_ep_corr_dist = correlation(snp_temp['DE'].values, snp_temp['EP'].values)\n",
    "    \n",
    "#     print(f\"Correlation distance between DP and DE: {dp_de_corr_dist}\")\n",
    "#     print(f\"Correlation distance between DP and EP: {dp_ep_corr_dist}\")\n",
    "#     print(f\"Correlation distance between DE and EP: {de_ep_corr_dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
